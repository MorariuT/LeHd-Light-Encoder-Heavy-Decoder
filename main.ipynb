{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52a0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "\n",
    "# # ----------------------------\n",
    "# #  UNET Autoencoder Definition\n",
    "# # ----------------------------\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ResNet18AutoEncoder(nn.Module):\n",
    "#     def __init__(self, pretrained=True):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # -----------------\n",
    "#         #  Encoder: ResNet-18\n",
    "#         # -----------------\n",
    "#         resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Extract layers up to layer4 (final conv output)\n",
    "#         self.enc1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # 64x128x128\n",
    "#         self.pool = resnet.maxpool  # 64x64x64\n",
    "#         self.enc2 = resnet.layer1  # 64x64x64\n",
    "#         self.enc3 = resnet.layer2  # 128x32x32\n",
    "#         self.enc4 = resnet.layer3  # 256x16x16\n",
    "#         self.enc5 = resnet.layer4  # 512x8x8\n",
    "\n",
    "#         # Bottleneck conv to 4x4\n",
    "#         self.bottleneck = nn.Conv2d(512, 512, 3, stride=2, padding=1)  # 512x4x4\n",
    "\n",
    "#         # -----------------\n",
    "#         #  Decoder\n",
    "#         # -----------------\n",
    "#         self.up1 = nn.ConvTranspose2d(512, 512, 2, stride=2)  # 512x8x8\n",
    "#         self.dec1 = nn.Conv2d(512 + 512, 512, 3, padding=1)  # skip from enc5\n",
    "\n",
    "#         self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)  # 256x16x16\n",
    "#         self.dec2 = nn.Conv2d(256 + 256, 256, 3, padding=1)  # skip from enc4\n",
    "\n",
    "#         self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)  # 128x32x32\n",
    "#         self.dec3 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "#         self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)  # 64x64x64\n",
    "#         self.dec4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "#         self.up5 = nn.ConvTranspose2d(64, 64, 2, stride=2)  # 64x128x128\n",
    "#         self.dec5 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "#         self.up6 = nn.ConvTranspose2d(64, 3, 2, stride=2)  # 3x256x256\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         e1 = self.enc1(x)     # 64x128x128\n",
    "#         e2 = self.enc2(self.pool(e1))  # 64x64x64\n",
    "#         e3 = self.enc3(e2)    # 128x32x32\n",
    "#         e4 = self.enc4(e3)    # 256x16x16\n",
    "#         e5 = self.enc5(e4)    # 512x8x8\n",
    "\n",
    "#         # Bottleneck\n",
    "#         b = self.bottleneck(e5)  # 512x4x4\n",
    "\n",
    "#         # Decoder with skip connections from e5 and e4\n",
    "#         d1 = F.relu(self.dec1(torch.cat([self.up1(b), torch.zeros_like(e5)], dim=1)))  # 512x8x8\n",
    "#         d2 = F.relu(self.dec2(torch.cat([self.up2(d1), torch.zeros_like(e4)], dim=1))) # 256x16x16\n",
    "#         d3 = F.relu(self.dec3(self.up3(d2)))  # 128x32x32\n",
    "#         d4 = F.relu(self.dec4(self.up4(d3)))  # 64x64x64\n",
    "#         d5 = F.relu(self.dec5(self.up5(d4)))  # 64x128x128\n",
    "#         out = torch.sigmoid(self.up6(d5))     # 3x256x256\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# def live_train_unet():\n",
    "#     size = 256\n",
    "\n",
    "#     model = ResNet18AutoEncoder().cuda()\n",
    "#     model = torch.load(\"model_1.pth\", weights_only=False)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # preprocess webcam frame\n",
    "#         inp = cv2.resize(frame, (size, size))\n",
    "#         inp_tensor = torch.tensor(inp).permute(2,0,1).float().cuda()/255.0\n",
    "#         inp_tensor = inp_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ------- TRAIN ON THIS FRAME -------\n",
    "#         model.train()\n",
    "#         out = model(inp_tensor)\n",
    "#         loss = loss_fn(out, inp_tensor)\n",
    "\n",
    "#         opt.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         # -----------------------------------\n",
    "\n",
    "#         # show reconstruction\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             decoded = model(inp_tensor)[0].cpu().permute(1,2,0).numpy()\n",
    "\n",
    "#         decoded = (decoded * 255).astype(np.uint8)\n",
    "#         combined = np.hstack((inp, decoded))\n",
    "\n",
    "#         cv2.putText(combined, f\"Loss: {loss.item():.4f}\", (10,20),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "\n",
    "#         cv2.imshow(\"Training (Left) | Reconstructed (Right)\", combined)\n",
    "\n",
    "#         if cv2.waitKey(1) & 0xFF == 27:\n",
    "#             torch.save(model, \"model_1.pth\")\n",
    "#             break  # ESC ends\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# # Run it\n",
    "# # live_train_unet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2c66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# live_train_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ce2bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.functional as F\n",
    "\n",
    "class ResNet18AutoEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # -----------------\n",
    "        # ENCODER (ResNet18)\n",
    "        # -----------------\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.enc1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # 64 x 128\n",
    "        self.pool = resnet.maxpool                                       # 64 x 64\n",
    "        self.enc2 = resnet.layer1                                        # 64 x 64\n",
    "        self.enc3 = resnet.layer2                                        # 128 x 32\n",
    "        self.enc4 = resnet.layer3                                        # 256 x 16\n",
    "        self.enc5 = resnet.layer4                                        # 512 x 8\n",
    "\n",
    "        # -----------------\n",
    "        # Bottleneck -> 4×4\n",
    "        # -----------------\n",
    "        self.bottleneck = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
    "\n",
    "        # -----------------\n",
    "        # SKIP PROJECTIONS\n",
    "        # (MAKE CHANNELS MATCH)\n",
    "        # -----------------\n",
    "        self.proj5 = nn.Conv2d(512, 512, 1)   # matches d1\n",
    "        self.proj4 = nn.Conv2d(256, 256, 1)   # matches d2\n",
    "        self.proj3 = nn.Conv2d(128, 128, 1)   # matches d3\n",
    "        self.proj2 = nn.Conv2d(64, 64, 1)     # matches d4\n",
    "        self.proj1 = nn.Conv2d(64, 64, 1)     # matches d5\n",
    "\n",
    "        # -----------------\n",
    "        # DECODER\n",
    "        # -----------------\n",
    "        self.up1 = nn.ConvTranspose2d(512, 512, 2, stride=2)          # 4→8\n",
    "        self.dec1 = nn.Conv2d(512 + 512, 512, 3, padding=1)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)          # 8→16\n",
    "        self.dec2 = nn.Conv2d(256 + 256, 256, 3, padding=1)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)          # 16→32\n",
    "        self.dec3 = nn.Conv2d(128 + 128, 128, 3, padding=1)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)           # 32→64\n",
    "        self.dec4 = nn.Conv2d(64 + 64, 64, 3, padding=1)\n",
    "\n",
    "        self.up5 = nn.ConvTranspose2d(64, 64, 2, stride=2)            # 64→128\n",
    "        self.dec5 = nn.Conv2d(64 + 64, 64, 3, padding=1)\n",
    "\n",
    "        self.up6 = nn.ConvTranspose2d(64, 3, 2, stride=2)             # 128→256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --------- ENCODER ----------\n",
    "        e1 = self.enc1(x)             # 64 x 128\n",
    "        e2 = self.enc2(self.pool(e1)) # 64 x 64\n",
    "        e3 = self.enc3(e2)            # 128 x 32\n",
    "        e4 = self.enc4(e3)            # 256 x 16\n",
    "        e5 = self.enc5(e4)            # 512 x 8\n",
    "\n",
    "        # --------- BOTTLENECK ----------\n",
    "        b = self.bottleneck(e5)       # 512 x 4\n",
    "\n",
    "        # --------- DECODER ----------\n",
    "        d1 = self.up1(b)\n",
    "        d1 = torch.cat([d1, torch.zeros_like(e5)], dim=1)\n",
    "        d1 = F.relu(self.dec1(d1))\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, torch.zeros_like(e4)], dim=1)\n",
    "        d2 = F.relu(self.dec2(d2))\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, torch.zeros_like(e3)], dim=1)\n",
    "        d3 = F.relu(self.dec3(d3))\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, torch.zeros_like(e2)], dim=1)\n",
    "        d4 = F.relu(self.dec4(d4))\n",
    "\n",
    "        d5 = self.up5(d4)\n",
    "        d5 = torch.cat([d5, torch.zeros_like(e1)], dim=1)\n",
    "        d5 = F.relu(self.dec5(d5))\n",
    "\n",
    "        out = torch.sigmoid(self.up6(d5))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d39b7",
   "metadata": {},
   "source": [
    "extra-low -> 8KB / frame\n",
    "low -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afeb18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def train_on_video():\n",
    "    size = 256\n",
    "    frame_itr = 0;\n",
    "\n",
    "    save_frame = [1, 1300, 1400, 1500, 1600, 1700]\n",
    "    type_ = \"t\"\n",
    "\n",
    "    model = ResNet18AutoEncoder().cuda()\n",
    "\n",
    "    # load model if exists\n",
    "    try:\n",
    "        model = torch.load(\"model_2.pth\", weights_only=False)\n",
    "        print(\"Loaded saved model.\")\n",
    "    except:\n",
    "        print(\"No saved model found, starting fresh.\")\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # VIDEO INPUT INSTEAD OF WEBCAM\n",
    "    cap = cv2.VideoCapture(\"movie3.mp4\")\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open movie.mp4\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(frame_itr)\n",
    "        frame_itr += 1\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "\n",
    "        # preprocess frame\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        inp = cv2.resize(frame, (size, size)).copy()\n",
    "        inp_tensor = torch.tensor(inp).permute(2,0,1).float().cuda() / 255.0\n",
    "        inp_tensor = inp_tensor.unsqueeze(0)\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        out = model(inp_tensor)\n",
    "        loss = loss_fn(out, inp_tensor)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            decoded = model(inp_tensor)[0].cpu().permute(1,2,0).numpy()\n",
    "\n",
    "        decoded_uint8 = np.clip(decoded*255, 0, 255).astype(np.uint8)\n",
    "        # assume inp and decoded_uint8 are both HxWx3 uint8 RGB images\n",
    "\n",
    "        # convert decoded to BGR for OpenCV display\n",
    "        decoded_bgr = cv2.cvtColor(decoded_uint8, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # create a horizontal separator bar (optional)\n",
    "        bar_height = 5\n",
    "        bar = np.zeros((bar_height, inp.shape[1], 3), dtype=np.uint8)  # black bar\n",
    "\n",
    "        # stack vertically: input on top, bar, then decoded\n",
    "        combined = np.vstack((inp, bar, decoded_uint8))\n",
    "\n",
    "        # add optional labels on top of each frame\n",
    "        top_height = 40\n",
    "        top = np.zeros((top_height, combined.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "        \n",
    "\n",
    "        # cv2.putText(top, \"ORIGINAL\", (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "        # cv2.putText(top, \"DECODED\", (10, top.shape[1]//2 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "\n",
    "        # final frame with labels\n",
    "        # final = np.vstack((top, combined))\n",
    "\n",
    "        cv2.imshow(\"Vertical Stack: Input over Decoded\", combined)\n",
    "\n",
    "        if(frame_itr in save_frame):\n",
    "            cv2.imwrite(\"frame_\" + str(frame_itr) + \"_\" + type_ + \".jpg\", decoded_uint8)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            print(\"Saving model...\")\n",
    "            torch.save(model, \"model_2.pth\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Run it\n",
    "# train_on_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da551faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "train_on_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c9acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
